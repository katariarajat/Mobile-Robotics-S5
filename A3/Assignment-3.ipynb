{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: ICP + Non-linear least squares optimization\n",
    "\n",
    "TEAM-NAME: Doraemon\n",
    "\n",
    "YOUR-ID: 2019101020, 2019101105\n",
    "\n",
    "YOUR-NAME: Rajat Kumar, Ashwin Mittal\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* You are not allowed to use any external libraries (other than ones being imported below).\n",
    "* The deadline for this assignment is **15-09-21** at 11:55pm.\n",
    "* Plagiarism is **strictly prohibited**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Linear Least Squares Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Gradient Descent\n",
    "Implement the gradient descent algorithm using numpy and what you have learned from class to solve for the parameters of a gaussian distribution.\n",
    "To understand the task in more detail and look at a worked through example, checkout the subsequent section. You have to implement the same using just numpy functions. You can refer to [Shubodh's notes](https://www.notion.so/saishubodh/From-linear-algebra-to-non-linear-weighted-least-squares-optimization-13cf17d318be4d45bb8577c4d3ea4a02) on the same to get a better grasp of the concept before implementing it.\n",
    "* Experiment with the number of iterations.\n",
    "* Experiment with the learning rate.\n",
    "* Experiment with the tolerance.\n",
    "\n",
    "Display your results using matplotlib by plotting graphs for \n",
    "* The cost function value vs the number of iterations\n",
    "* The Ground Truth data values and the predicted data values.\n",
    "\n",
    "Your plots are expected to contain information similar to the plot below:\n",
    "\n",
    "<!-- <figure> -->\n",
    "<img src='./helpers/sample_plt.png' alt=drawing width=500 height=600>\n",
    "\n",
    "<!-- <figcaption align='center'><b>A sample plot, you can use your own plotting template</b></figcaption>\n",
    "</figure> -->\n",
    "<!-- head over to [this page](https://saishubodh.notion.site/Non-Linear-Least-Squares-Solved-example-Computing-Jacobian-for-a-Gaussian-Gradient-Descent-7fd11ebfee034f8ca89cc78c8f1d24d9) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked out Example using Gradient Descent\n",
    "\n",
    "A Gaussian distribution parametrized by $a,m,s$ is given by:\n",
    "\n",
    "$$ y(x;a,m,s)=a \\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right) \\tag{1}$$\n",
    "\n",
    "### Jacobian of Gaussian\n",
    "\n",
    "$$\\mathbf{J}_y=\\left[\\frac{\\partial y}{\\partial a} \\quad \\frac{\\partial y}{\\partial m} \\quad \\frac{\\partial y}{\\partial s}\\right] \\\\\n",
    "= \\left[ \\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right); \\frac{a (x-m)}{s^2} \\exp\\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right);  \\frac{a (x-m)^2}{s^3}\\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right)\\right]$$\n",
    "\n",
    "## Problem at hand\n",
    "\n",
    "> Given a set of observations $y_{obs}$ and $x_{obs}$ we want to find the optimum parameters $a,m,s$ which best fit our observations given an initial estimate.\n",
    "\n",
    "Our observations would generally be erroneous and given to us, but for the sake of knowing how good our model is performing, let us generate the observations ourselves by assuming the actual \"actual\" parameter values as $a_{gt}=10; m_{gt} =0; s_{gt} =20$ ($gt$ stands for ground truth). We will try to estimate these values based on our observations and let us see how close we get to \"actual\" parameters. Note that in reality we obviously don't have these parameters as that is exactly what we want to estimate in the first place. So let us consider the following setup, we have:\n",
    "\n",
    "- Number of observations, $num\\_obs = 50$\n",
    "- Our 50 set of observations would be\n",
    "    - $x_{obs} = np.linspace(-25,25, num\\_obs)$\n",
    "    - $y_{obs} = y(x_{obs};a_{gt},m_{gt},s_{gt})$  from $(1)$\n",
    "\n",
    "Reference:\n",
    "\n",
    "→[linspace](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html)\n",
    "\n",
    "- Say we are given initial estimate as:\n",
    "\n",
    "    $$a_0=10; \\quad m_0=13; \\quad s_0=19.12$$\n",
    "\n",
    "### Residual and error to be minimized\n",
    "\n",
    "Okay, now we have set of observations and an initial estimate of parameters. We would now want to minimize an error that would give us optimum parameters.\n",
    "\n",
    "The $residual$ would be given by\n",
    "\n",
    "$$ r(a,m,s) = \\left[ a \\exp \\left(\\frac{-(x_{obs}-m)^{2}}{2 s^{2}}\\right) - y_{obs}\\ \\right]$$\n",
    "\n",
    "where we'd want to minimize $\\|r\\|^2$. Note that $r$ is a non-linear function in $(a,m,s)$.\n",
    "\n",
    "Also, note that since $y$ (and $x$) are observations in the above equation, after simplification, we get $\\mathbf{J}_r = \\mathbf{J}_y$ [above](https://www.notion.so/c9e6f71b67a44bb8b366df2fccfc12d0) (since $y_{obs}$ is a constant).\n",
    "\n",
    "Let us apply Gradient Descent method for minimization here. From [Table I](https://www.notion.so/From-linear-algebra-to-non-linear-weighted-least-squares-optimization-13cf17d318be4d45bb8577c4d3ea4a02),  \n",
    "\n",
    "$$\\Delta \\mathbf{k} = - \\alpha \\mathbf{J_F} = -\\alpha \\mathbf{J}_r^{\\top} {r}(\\mathbf{k})$$\n",
    "\n",
    "Note that $\\mathbf{J_F}$ is the Jacobian of \"non-linear least squares\" function $\\mathbf{F}$ while $\\mathbf{J}_r$ is the Jacobian of the residual. \n",
    "\n",
    "where $\\mathbf{k}$ is $[a,m,s]^T$. \n",
    "\n",
    "- Some hyperparameters:\n",
    "    - Learning rate, $lr = 0.01$\n",
    "    - Maximum number of iterations, $num\\_iter=200$\n",
    "    - Tolerance, $tol = 1e-15$\n",
    "\n",
    "## Solution for one iteration\n",
    "\n",
    "To see how each step looks like, let us solve for 1 iteration and for simpler calculations, assume we have 3 observations, \n",
    "\n",
    "$$x_{obs}= \\left[ -25, 0, 25 \\right]^T, y_{obs} = \\left[  4.5783, 10, 4.5783 \\right]^T. $$\n",
    "\n",
    "With our initial estimate as $\\mathbf{k_0} = [a_0=10, \\quad m_0=13, \\quad s_0=19.12]^T$, the residual would be \n",
    "\n",
    "$$ r(a_0,m_0,s_0) = \\left[ a_0 \\exp \\left(\\frac{-(x_{obs}-m_0)^{2}}{2 s_0^{2}}\\right) - y_{obs}\\ \\right]$$\n",
    "\n",
    "Therefore, $r=[-3.19068466, -2.0637411 , 3.63398058]^T$.\n",
    "\n",
    "### Gradient Computation\n",
    "\n",
    "Gradient, $\\mathbf{J_F}$=\n",
    "\n",
    "$$\\mathbf{J_r}^{\\top} \\mathbf{r}(\\mathbf{k})$$\n",
    "\n",
    "We have calculated residual already [above](https://www.notion.so/c9e6f71b67a44bb8b366df2fccfc12d0), let us calculate the Jacobian $\\mathbf{J_r}$.\n",
    "\n",
    "$$\\mathbf{J}_r\n",
    "= \\left[ \\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right); \\frac{a (x-m)}{s^2} \\exp\\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right);  \\frac{a (x-m)^2}{s^3}\\exp \\left(\\frac{-(x-m)^{2}}{2 s^{2}}\\right)\\right]$$\n",
    "\n",
    "$$\\implies \\mathbf{J_r} = \\left[ \\begin{array}{rrr}0.1387649 & 0.79362589, & 0.82123142 \\\\-0.14424057 & -0.28221715  & 0.26956967 \\\\0.28667059 & 0.19188405, & 0.16918599\\end{array}\\right]$$\n",
    "\n",
    "So ,\n",
    "\n",
    "$$\\mathbf{J_F} = \\mathbf{J_r}^{\\top} \\mathbf{r}(\\mathbf{k})$$\n",
    "\n",
    "$$\\mathbf{r}(\\mathbf{k}) =  \\left[ \\begin{array}{r}-3.19068466 \\\\ -2.0637411 \\\\ 3.63398058 \\end{array} \\right]$$\n",
    "\n",
    "$$ \\begin{aligned} \\implies \\mathbf{J_F} = \\left[ \\begin{array}{r} 0.89667553 \\\\ -1.25248392 \\\\-2.56179392\\end{array} \\right] \\end{aligned}$$\n",
    "\n",
    "### Update step\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{k} = - \\alpha \\mathbf{J_F} \\\\\n",
    "\\mathbf{k}^{t+1} = \\mathbf{k}^t + \\Delta \\mathbf{k}\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ our learning rate is 0.01.\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{k} = - \\alpha\\times\\left[ \\begin{array}{r} \n",
    "0.89667553 \\\\ -1.25248392 \\\\-2.56179392\n",
    "\\end{array} \\right] = \\left[ \\begin{array}{r}\n",
    "-0.00896676 \\\\ 0.01252484 \\\\0.02561794\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{k}^{1} = \\mathbf{k}^{0} + \\Delta \\mathbf{k} \\\\ \\left[\\begin{array}{r} 10 \\\\ 13 \\\\ 19.12 \\end{array}\\right] + \\left[\\begin{array}{r} -0.00896676 \\\\ 0.01252484 \\\\0.02561794 \\end{array}\\right] = \\left[\\begin{array}{c} 9.99103324 \\\\ 13.01252484 \\\\ 19.14561794 \\end{array} \\right]\n",
    "$$\n",
    "\n",
    "With just one iteration with very few observations, we can see that we have gotten *slightly* more closer to our GT parameter  $a_{gt}=10; m_{gt} =0; s_{gt} =20$. Our initial estimate was $[a_0=10, \\quad m_0=13, \\quad s_0=19.12]$. However, the above might not be noticeable enough: Hence you need to code it for more iterations and convince yourself as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.func import make_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Partial Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def da(x, a, m, s):\n",
    "    d = np.exp(-((x - m) ** 2) / (2 * s ** 2))\n",
    "    return d\n",
    "\n",
    "\n",
    "def dm(x, a, m, s):\n",
    "    d = a * ((x - m) / s ** 2) * (np.exp(-((x - m) ** 2) / (2 * s ** 2)))\n",
    "    return d\n",
    "\n",
    "\n",
    "def ds(x, a, m, s):\n",
    "    d = a * ((x - m) ** 2 / s ** 3) * (np.exp(-((x - m) ** 2) / (2 * s ** 2)))\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs = 50\n",
    "lr = 0.01\n",
    "num_iter = 200\n",
    "tol = 10 ** -15\n",
    "w = np.array([10, 13, 19.12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-25, 25, num_obs)\n",
    "y = make_gaussian(x, 10, 0, 20)\n",
    "func = make_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ps(x, y, w, cost):\n",
    "    plt.subplot(121)\n",
    "    plt.plot(x, y, \"-\", label=\"Ground Truth\")\n",
    "    plt.plot(x, func(x, *w), \"-o\", label=\"Predicted Value\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Actual vs. Predicted\")\n",
    "    plt.legend()\n",
    "    plt.subplot(122)\n",
    "    plt.plot(cost, \"-o\")\n",
    "    plt.title(\"Loss vs. Iterations\")\n",
    "    plt.xlabel(\"number of iterations\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = []\n",
    "for iteration in range(num_iter):\n",
    "    r = make_gaussian(x, *w) - y\n",
    "    cost.append(sum(r ** 2))\n",
    "    J = np.vstack((da(x, *w), dm(x, *w), ds(x, *w)))\n",
    "    dw = np.dot(J, r.T)\n",
    "    w = w - lr * dw\n",
    "    print(\"Sum Squared Error:\", cost[-1])\n",
    "    plot_ps(x, y, w, cost)\n",
    "    if np.linalg.norm(dw) <= tol:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying Number of Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iterations in [100, 200, 500, 1000]:\n",
    "    cost = []\n",
    "    w = np.array([10, 13, 19.12])\n",
    "    for iteration in range(iterations):\n",
    "        r = make_gaussian(x, *w) - y\n",
    "        cost.append(sum(r ** 2))\n",
    "        J = np.vstack((da(x, *w), dm(x, *w), ds(x, *w)))\n",
    "        dw = np.dot(J, r.T)\n",
    "        w = w - lr * dw\n",
    "        if np.linalg.norm(dw) <= tol:\n",
    "            break\n",
    "    print(\"Sum Squared Error:\", cost[-1])\n",
    "    plot_ps(x, y, w, cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate in [0.001, 0.003, 0.01]:\n",
    "    cost = []\n",
    "    w = np.array([10, 13, 19.12])\n",
    "    for iteration in range(num_iter):\n",
    "        r = make_gaussian(x, *w) - y\n",
    "        cost.append(sum(r ** 2))\n",
    "        J = np.vstack((da(x, *w), dm(x, *w), ds(x, *w)))\n",
    "        dw = np.dot(J, r.T)\n",
    "        w = w - learning_rate * dw\n",
    "        if np.linalg.norm(dw) <= tol:\n",
    "            break\n",
    "    print(\"Sum Squared Error:\", cost[-1])\n",
    "    plot_ps(x, y, w, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the Tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tolerance in [10 ** -3, 10 ** -9, 10 ** -27]:\n",
    "    cost = []\n",
    "    w = np.array([10, 13, 19.12])\n",
    "    for iteration in range(1000):\n",
    "        r = make_gaussian(x, *w) - y\n",
    "        cost.append(sum(r ** 2))\n",
    "        J = np.vstack((da(x, *w), dm(x, *w), ds(x, *w)))\n",
    "        dw = np.dot(J, r.T)\n",
    "        w = w - lr * dw\n",
    "        if np.linalg.norm(dw) <= tolerance:\n",
    "            break\n",
    "    print(\"Sum Squared Error:\", cost[-1])\n",
    "    plot_ps(x, y, w, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2: Another Non-Linear function\n",
    "Now that you've got the hang of computing the jacobian matrix for a non-linear function via the aid of an example, try to compute the jacobian of a secondary gaussian function by carrying out steps similar to what has been shown above. The function is plotted below:\n",
    "<img src='./helpers/non_linear.png' alt=drawing width=500 height=600>\n",
    "Using the computed jacobian, optimise for the four parameters using gradient descent, where the parameters to be estimated are: \n",
    "\n",
    "$p_1$ = 2,  $p_2$ = 8,  $p_3$ = 4,  $p_4$ = 8. \n",
    "\n",
    "Do this for $x_{obs} = np.linspace(-20,30, num\\_obs)$,\n",
    "where $num\\_obs$ is 50.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.func import make_non_linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-20, 30, num_obs)\n",
    "y = make_non_linear(x, 2, 8, 4, 8)\n",
    "func = make_non_linear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([5, 5, 5, 5])\n",
    "lr = 0.000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Partial Derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_p1(x, p1, p2, p3, p4):\n",
    "    d = np.exp(-x / p2)\n",
    "    return d\n",
    "\n",
    "\n",
    "def d_p2(x, p1, p2, p3, p4):\n",
    "    d = x * p1 * np.exp(-x / p2) / (p2 ** 2)\n",
    "    return d\n",
    "\n",
    "\n",
    "def d_p3(x, p1, p2, p3, p4):\n",
    "    d = np.sin(x / p4)\n",
    "    return d\n",
    "\n",
    "\n",
    "def d_p4(x, p1, p2, p3, p4):\n",
    "    d = -x * p3 * np.cos(x / p4) / (p4 ** 2)\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = []\n",
    "for iteration in range(15000):\n",
    "    r = make_non_linear(x, *w) - y\n",
    "    cost.append(sum(r ** 2))\n",
    "#     print(\"Sum Squared Error:\", cost[-1])\n",
    "#     plot_ps(x, y, w, cost)\n",
    "    J = np.vstack((d_p1(x, *w), d_p2(x, *w), d_p3(x, *w), d_p4(x, *w)))\n",
    "    dw = np.dot(J, r.T)\n",
    "    w = w - lr * dw\n",
    "    if np.linalg.norm(dw) <= tol:\n",
    "        break\n",
    "print(\"Sum Squared Error:\", cost[-1])\n",
    "plot_ps(x, y, w, cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the above optimization problem, gradient descent is stuck on a local minimum.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3: Different Optimizers\n",
    "\n",
    "Replace gradient descent with Gauss-Newton and Levenberg Marquardt algorithms and repeat question 1.1. \n",
    "\n",
    "To quickly recap, Gauss-Newton and Levenberg Marquardt are alternate update rules to the standard gradient descent. Gauss Newton updates work as:\n",
    "\n",
    "$$\\delta x = -(J^TJ)^{-1}J^Tf(x)$$\n",
    "\n",
    "Levenberg Marquardt lies somewhere between Gauss Newton and Gradient Descent algorithms by blending the two formulations. As a result, when at a steep cliff, LM takes small steps to avoid overshooting, and when at a gentle slope, LM takes bigger steps:\n",
    "\n",
    "\n",
    "$$\\delta x = -(J^TJ + \\lambda I)^{-1}J^Tf(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "   * 1. How does the choice of initial estimate and learning rate affect convergence? Observations and analysis from repeated runs with modified hyperparameters will suffice.\n",
    "   * 2. Do you notice any difference between the three optimizers? Why do you think that is? (If you are unable to see a clear trend, what would you expect in general based on what you know about them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-25, 25, num_obs)\n",
    "y = make_gaussian(x, 10, 0, 20)\n",
    "func = make_gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([10, 13, 19.12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Newton Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = []\n",
    "for iteration in range(num_iter):\n",
    "    r = make_gaussian(x, *w) - y\n",
    "    cost.append(sum(r ** 2))\n",
    "    J = np.vstack((da(x, *w), dm(x, *w), ds(x, *w)))\n",
    "    dw = np.linalg.inv(J @ J.T) @ J @ r \n",
    "    w = w - dw\n",
    "    print(\"Sum Squared Error:\", cost[-1])\n",
    "    plot_ps(x, y, w, cost)\n",
    "    if np.linalg.norm(dw) <= tol:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the Number of Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iterations in [100, 200, 500, 1000]:\n",
    "    cost = []\n",
    "    w = np.array([10, 13, 19.12])\n",
    "    for iteration in range(iterations):\n",
    "        r = make_gaussian(x, *w) - y\n",
    "        cost.append(sum(r ** 2))\n",
    "        J = np.vstack((da(x, *w), dm(x, *w), ds(x, *w)))\n",
    "        dw = np.linalg.inv(J @ J.T) @ J @ r \n",
    "        w = w - dw\n",
    "        if np.linalg.norm(dw) <= tol:\n",
    "            break\n",
    "    print(\"Sum Squared Error:\", cost[-1])\n",
    "    plot_ps(x, y, w, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the Tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tolerance in [10 ** -3, 10 ** -9, 10 ** -27]:\n",
    "    cost = []\n",
    "    w = np.array([10, 13, 19.12])\n",
    "    for iteration in range(num_iter):\n",
    "        r = make_gaussian(x, *w) - y\n",
    "        cost.append(sum(r ** 2))\n",
    "        J = np.vstack((da(x, *w), dm(x, *w), ds(x, *w)))\n",
    "        dw = np.linalg.inv(J @ J.T) @ J @ r \n",
    "        w = w - dw\n",
    "        if np.linalg.norm(dw) <= tolerance:\n",
    "            break\n",
    "    print(\"Sum Squared Error:\", cost[-1])\n",
    "    plot_ps(x, y, w, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([10, 13, 19.12])\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Levenberg-Marquardt Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = []\n",
    "for iteration in range(num_iter):\n",
    "    r = make_gaussian(x, *w) - y\n",
    "    cost.append(sum(r ** 2))\n",
    "    J = np.vstack((da(x, *w), dm(x, *w), ds(x, *w)))\n",
    "    dw = np.linalg.inv(J @ J.T + lr * np.identity(3)) @ J @ r \n",
    "    new_w = w - dw\n",
    "    new_r = make_gaussian(x, *new_w) - y\n",
    "    new_err = sum(new_r ** 2)\n",
    "    if new_err > cost[-1]:\n",
    "        lr *= 10\n",
    "    else:\n",
    "        lr /= 10\n",
    "        w = new_w\n",
    "    print(\"Sum Squared Error:\", cost[-1])\n",
    "    plot_ps(x, y, w, cost)\n",
    "    if np.linalg.norm(dw) <= tol:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the Number of Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fde823784652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19.12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_gaussian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "for iterations in [100, 200, 500, 1000]:\n",
    "    cost = []\n",
    "    lr = 0.01\n",
    "    w = np.array([10, 13, 19.12])\n",
    "    for iteration in range(iterations):\n",
    "        r = make_gaussian(x, *w) - y\n",
    "        cost.append(sum(r ** 2))\n",
    "        J = np.vstack((da(x, *w), dm(x, *w), ds(x, *w)))\n",
    "        dw = np.linalg.inv(J @ J.T + lr * np.identity(3)) @ J @ r \n",
    "        new_w = w - dw\n",
    "        new_r = make_gaussian(x, *w) - y\n",
    "        new_err = sum(new_r ** 2)\n",
    "        if new_err >= cost[-1]:\n",
    "            lr *= 10\n",
    "        else:\n",
    "            lr /= 10\n",
    "            w = new_w\n",
    "        if np.linalg.norm(dw) <= tol:\n",
    "            break\n",
    "    print(\"Sum Squared Error:\", cost[-1])\n",
    "    plot_ps(x, y, w, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_rate in [0.001, 0.003, 0.01]:\n",
    "    cost = []\n",
    "    w = np.array([10, 13, 19.12])\n",
    "    for iteration in range(num_iter):\n",
    "        r = make_gaussian(x, *w) - y\n",
    "        cost.append(sum(r ** 2))\n",
    "        J = np.vstack((da(x, *w), dm(x, *w), ds(x, *w)))\n",
    "        dw = np.linalg.inv(J @ J.T + learning_rate * np.identity(3)) @ J @ r \n",
    "        new_w = w - dw\n",
    "        new_r = make_gaussian(x, *w) - y\n",
    "        new_err = sum(new_r ** 2)\n",
    "        if new_err > cost[-1]:\n",
    "            learning_rate *= 10\n",
    "        else:\n",
    "            learning_rate /= 10\n",
    "            w = new_w\n",
    "        if np.linalg.norm(dw) <= tol:\n",
    "            break\n",
    "    print(\"Sum Squared Error:\", cost[-1])\n",
    "    plot_ps(x, y, w, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the Tolerance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tolerance in [10 ** -3, 10 ** -9, 10 ** -27]:\n",
    "    cost = []\n",
    "    lr = 0.01\n",
    "    w = np.array([10, 13, 19.12])\n",
    "    for iteration in range(num_iter):\n",
    "        r = make_gaussian(x, *w) - y\n",
    "        cost.append(sum(r ** 2))\n",
    "        J = np.vstack((da(x, *w), dm(x, *w), ds(x, *w)))\n",
    "        dw = np.linalg.inv(J @ J.T + lr * np.identity(3)) @ J @ r \n",
    "        new_w = w - dw\n",
    "        new_r = make_gaussian(x, *w) - y\n",
    "        new_err = sum(new_r ** 2)\n",
    "        if new_err > cost[-1]:\n",
    "            lr *= 10\n",
    "        else:\n",
    "            lr /= 10\n",
    "            w = new_w\n",
    "        if np.linalg.norm(dw) <= tolerance:\n",
    "            break\n",
    "    print(\"Sum Squared Error:\", cost[-1])\n",
    "    plot_ps(x, y, w, cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The gradient method often exhibits linear convergence, i.e., f is locally linear. The choice of backtracking parameters has a noticeable but not dramatic effect on the convergence. The convergence rate depends greatly on the condition number of the Hessian. When the condition number is 1000 or more, the gradient method is so slow that it is useless in practice. It is very simple but rarely used in practice due to slow convergence. Sometimes loss starts increasing if the learning rate is too high. If suppose initialization is not that good gradient is vanishingly small, then there’ll barely be any update to the weight. The gradient descent method is the steepest descent method for the Euclidean norm.\n",
    "\n",
    "The non-convex nature of this error function makes iterative algorithms like gradient descent susceptible to poor Initialization or choice of hyperparameters.\n",
    "\n",
    "**Learning Rate:** While a higher learning rate makes the algorithm converge towards the minima faster, as shown by the figures, setting too high a learning rate causes the algorithm to misbehave and diverge—this is because the algorithm tends to miss and overshoot the minima.\n",
    "\n",
    "**Initialization:** An initialization that is far from the actual minima can severely affect the algorithm's performance. Depending on the gradients around the initialization point, it can either slow the convergence rate or even cause the algorithm to get stuck in a different local minimum if the gradients point towards it instead.\n",
    "\n",
    "**Number of Iterations:** From the figures, gradient descent takes larger leaps in the beginning and smaller ones as it closes in towards the minima. This stagnation means that although it still improves in each step, the improvements are minor and hence of not much significance after, in this case, 200 iterations.\n",
    "\n",
    "**Fewer Observations:** The number of observations affects gradient descent—as the quality of estimates and rate of convergence deteriorate with a decrease in the number of observations. A decrease in the number of observations flattens the cost function and reduces the gradients—making them difficult to follow for gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The Gauss-Newton method and the Levenberg-Marquardt Method has several very strong advantages over the gradient and steepest descent methods, i. e., fast convergence (at most six iterations in the quadratic phase) and affine invariance: insensitive to the choice of coordinates Scales well with problem size (only a few more steps are necessary between $R^{100}$ and $R^{10000}$). The performance is not dependent on the choice of the algorithm parameters. The main disadvantage is the cost of computation.\n",
    "\n",
    "The Levenberg-Marquardt (LM) algorithm is the most widely used optimization algorithm. It outperforms simple gradient descent and other conjugate gradient methods in a wide variety of problems. Simple gradient descent suffers from various convergence problems. Logically, we would like to take large steps down the gradient at locations where the gradient is small and, conversely, take small steps when the gradient is large so as not to rattle out of the minima. With the above update rule, we do just the opposite of this. Another issue is that the curvature of the error surface may not be the same in all directions. This situation can be improved upon by using curvature as well as gradient information, namely second derivatives. The main advantage of this technique is rapid convergence. However, the rate of convergence is sensitive to the starting location. Levenberg proposed an algorithm based on this observation, whose update rule is a blend of both the algorithms. It is to be noted that while the LM method is in no way optimal but is just a heuristic, it works extremely well in practice. The only flaw is its need for matrix inversion as part of the update. Even though the inverse is usually implemented using clever pseudo-inverse methods such as singular value decomposition, the cost of the update becomes prohibitive after the model size increases to a few thousand parameters. However, this method is much faster than vanilla gradient descent for moderately sized models (of a few hundred parameters). While optimizing a non-linear least-squares formulation, we linearise the cost function in the neighborhood of our current estimate, using the gradients to estimate the next guess. While gradient descent moves in the opposite direction of the gradient, Gauss-Newton uses the closed-form solution of the locally-linear least-squares approximation to arrive at the next estimate. Levenberg-Marquardt balances these two approaches using a damping factor, which also imparts numerical stability by ensuring that the matrix is fully rank and, hence, invertible. We start with and decrease it by a factor of  10 every time the loss decreases and making it ten times every time the loss increases. This causes Levenberg-Marquardt to follow the gradients when it is far away from the minima by increasing the gradient-descent-like character, and quickly move towards the minima of the local curvature when it is closer to the minima by increasing the Gauss-Newton-like character of the algorithm.\n",
    "\n",
    "**Initialization:** Both gradient descent and Gauss-Newton fail to estimate the parameters correctly when the Initialization is not close to the actual minima. Following the curvature takes Gauss-Newton to a different local minimum. Hence its estimates are off by a huge amount. Gradient descent, despite producing an even higher cost, makes an estimate closer to the actual. However, it is unable to converge in a limited number of iterations. Levenberg-Marquardt, which initially follows the gradient to move in the right direction and then follows the curvature to converge quickly, is the only one that can estimate the parameters accurately.\n",
    "\n",
    "**Number of Iterations:** Both Gauss-Newton and Levenberg-Marquardt converge much quicker than gradient-descent. This is because they move to the minima of the local approximation of the function in each iteration instead of just moving along the gradient by a constant amount.\n",
    "\n",
    "**Fewer Observations:** While both Gauss-Newton and Levenberg-Marquardt are resilient to a fewer number of observations, gradient descent's performance gets adversely affected. This is because a decrease in the number of observations flattens the cost function and decreases the gradients—making it difficult for gradient descent to converge quickly (see figure below, compared to the one above on the same scale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Iterative Closest Point\n",
    "\n",
    "In this subsection, we will code the Iterative Closest Point algorithm to find the alignment between two point clouds without known correspondences. The point cloud that you will be using is the same as the one that you used in Assignment 1.\n",
    "\n",
    "## 2.1: Procrustes alignment\n",
    "\n",
    "1. Write a function that takes two point clouds as input wherein the corresponding points between the two point clouds are located at the same index and returns the transformation matrix between them.\n",
    "2. Use the bunny point cloud and perform the procrustes alignment between the two bunnies. Compute the absolute alignment error after aligning the two bunnies.\n",
    "3. Make sure your code is modular as we will use this function in the next sub-part.\n",
    "4. Prove mathematically why the Procrustes alignment gives the best aligning transform between point clouds with known correspondences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globlal Variable \n",
    "bunny_file = \"./data/bunny.ply\"\n",
    "\n",
    "# just for visualisation purpose\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "import copy\n",
    "import open3d as o3d\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bunny_pcd = o3d.io.read_point_cloud(bunny_file)\n",
    "bunny_pcd = bunny_pcd.voxel_down_sample(voxel_size=0.005)\n",
    "o3d.visualization.draw_geometries([bunny_pcd])\n",
    "print(bunny_pcd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Finding centroid of point cloud\n",
    "# Inputs:\n",
    "# pcd: (N x 3) matrix of points in a point cloud\n",
    "# Returns:\n",
    "# mean: (1 x 3) matrix of the centroid of the point cloud\n",
    "def get_centroid(pcd):\n",
    "    return np.mean(pcd, 0)\n",
    "\n",
    "\n",
    "# Purpose: Computes root-mean-square error between two point cloud\n",
    "# Inputs:\n",
    "# pcd1: N x 3 matrix of points in starting point cloud\n",
    "# pcd2: N x 3 matrix of points in target point cloud\n",
    "# Returns: RMSE\n",
    "def getRmse(pcd1, pcd2):\n",
    "    dist = pcd1 - pcd2\n",
    "    dist_sq = np.square(dist)\n",
    "    err = np.sqrt(np.sum(dist_sq) / len(pcd1))\n",
    "    return err\n",
    "\n",
    "\n",
    "# Purpose: For given correspondences between two poincloud,\n",
    "# centre both on their centroid and perform procrustes alignment.\n",
    "# Inputs:\n",
    "# pcd1: N x 3 matrix of points in starting point cloud\n",
    "# pcd2: N x 3 matrix of points in target point cloud\n",
    "# correspondences: if none - same index\n",
    "# else - array of size N which stores the indices\n",
    "# Returns:\n",
    "# (3x3) rotation matrix to rotate and align pcd1 to pcd2 after\n",
    "# they have both been centered on their centroids.\n",
    "# (1 x 3) tranlation_matrix\n",
    "# final point cloud after alignment\n",
    "# RMSE between pcd2 and pcd1\n",
    "def procrustesAlignment(pcd1, pcd2, correspondences=[]):\n",
    "    if correspondences.__len__() <= 0:\n",
    "        correspondences = range(len(pcd1))\n",
    "    centroid_pcd1 =  np.mean(pcd1,0)\n",
    "    centroid_pcd2 =  np.mean(pcd2, 0)\n",
    "    p1 = pcd1 - centroid_pcd1\n",
    "    p2 = pcd2[correspondences, :] - centroid_pcd2\n",
    "    (U, _, VT) = np.linalg.svd((p2.T @ p1).T)\n",
    "    rotation_matrix = U @ VT\n",
    "    translation_matrix = centroid_pcd1.reshape(\n",
    "        (3, 1)\n",
    "    ) - rotation_matrix @ centroid_pcd2.reshape((3, 1))\n",
    "    transformation = np.eye(4)\n",
    "    transformation[:3, :3] = rotation_matrix\n",
    "    transformation[:3, 3] = translation_matrix.reshape((1, 3))\n",
    "    return transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transformation = np.eye(4)\n",
    "dim = 3  # number of dimensions of the points\n",
    "translation = 1  # max translation of the test set\n",
    "rotation = 1  # max rotation (radians) of the test set\n",
    "\n",
    "\n",
    "def rotation_matrix(axis, theta):\n",
    "    axis = axis / np.sqrt(np.dot(axis, axis))\n",
    "    a = np.cos(theta / 2.0)\n",
    "    b, c, d = -axis * np.sin(theta / 2.0)\n",
    "\n",
    "    return np.array(\n",
    "        [\n",
    "            [a * a + b * b - c * c - d * d, 2 * (b * c - a * d), 2 * (b * d + a * c)],\n",
    "            [2 * (b * c + a * d), a * a + c * c - b * b - d * d, 2 * (c * d - a * b)],\n",
    "            [2 * (b * d - a * c), 2 * (c * d + a * b), a * a + d * d - b * b - c * c],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "R = rotation_matrix(np.random.rand(dim), np.random.rand() * rotation)\n",
    "t = np.random.rand(dim) * translation\n",
    "\n",
    "test_transformation[:3, 3] = t\n",
    "test_transformation[:3, :3] = R\n",
    "print(test_transformation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd1 = copy.deepcopy(np.array(bunny_pcd.points))\n",
    "coord_1 = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "    size=0.06, origin=[0, 0, 0]\n",
    ")\n",
    "pcd2 = test_transformation @ np.vstack(\n",
    "    (copy.deepcopy(pcd1).T, np.ones((1, pcd1.__len__())))\n",
    ")\n",
    "coord_2 = o3d.geometry.TriangleMesh.create_coordinate_frame(\n",
    "    size=0.06, origin=[0, 0, 0]\n",
    ").transform(test_transformation)\n",
    "pcd2 = np.true_divide(pcd2.T[:, 0:3], pcd2.T[:, [-1]])\n",
    "transform = procrustesAlignment(pcd2, pcd1)\n",
    "print(\"pcd1 = \", pcd1)\n",
    "print(\"\\npcd2 = \", pcd2)\n",
    "print(\"\\ntransformation = \", transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obtained_pcd2 = (\n",
    "    transform @ np.vstack((copy.deepcopy(pcd1).T, np.ones((1, pcd1.__len__()))))\n",
    ")\n",
    "obtained_pcd2 = np.true_divide(obtained_pcd2.T[:, 0:3], obtained_pcd2.T[:, [-1]])\n",
    "print(\"Absolute Allignment Error = \", getRmse(pcd2, obtained_pcd2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checking_reverse = (\n",
    "    np.linalg.inv(transform) @ np.vstack((copy.deepcopy(pcd2).T, np.ones((1, pcd2.__len__()))))\n",
    ")\n",
    "checking_reverse = np.true_divide(checking_reverse.T[:, 0:3], checking_reverse.T[:, [-1]])\n",
    "print(\"Absolute Allignment Error = \", getRmse(checking_reverse, pcd1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1.4 proof in pdf 2.1.4.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: ICP alignment\n",
    "\n",
    "1. Write a function that takes two point clouds as input without known correspondences and perform the iterative closest point algorithm.\n",
    "2. Perform the ICP alignment between the two bunnies and plot their individual coordinate frames as done in class.\n",
    "3. Does ICP always give the correct alignment? Why or Why not?\n",
    "4. What are other variants of ICP and why are they helpful (you can look at point to plane ICP)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sum(pcdT):\n",
    "    return np.sum(pcdT,0)\n",
    "\n",
    "def get_argmin(D):\n",
    "    return np.argmin(D,1)\n",
    "# Purpose: Finding the nearest neighbors of pcd2 to points in pcd1,\n",
    "# given an estimate of the aligning matrix that aligns\n",
    "# pcd2 tp pcd1, as well as the centroids of those two point clouds, to\n",
    "# Inputs:\n",
    "# pcd1: N x 3 matrix of points in starting point cloud\n",
    "# pcd2: N x 3 matrix of points in target point cloud\n",
    "# rotation_matrix: Current estimate of rotation matrix for pcd2 w.r.t pcd1\n",
    "# Returns:\n",
    "# correspondence: An array of size N which stores the indices\n",
    "def nearestNeighbour(pcd1, pcd2):\n",
    "    p1 = (pcd1 - get_centroid(pcd1)).T\n",
    "    p2 = (pcd2 - get_centroid(pcd2)).T\n",
    "    AB = p2.T @ p1\n",
    "    XX = get_sum(p2*p2)\n",
    "    YY = get_sum(p1*p1)\n",
    "    D = (XX[:, np.newaxis] + YY[np.newaxis, :]) - 2 * AB\n",
    "    correspondence = get_argmin(D)\n",
    "    return correspondence\n",
    "\n",
    "\n",
    "# Purpose: Visualise the point cloud and point clout obtained after\n",
    "# performing transformation\n",
    "# Inputs:\n",
    "# points1: N x 3 matrix of points in the visualising point cloud which is starting  pcd.\n",
    "# points2: N x 3 matrix of points in the visualising point cloud which is target pcd.\n",
    "# transform: transformation matrix from points2 to points1\n",
    "# Returns: -\n",
    "def visualisePcd(points1, points2, coord_1, coord_2):\n",
    "    v3d = o3d.utility\n",
    "    v3d = v3d.Vector3dVector\n",
    "    pcd1 = o3d.geometry\n",
    "    pcd1 = pcd1.PointCloud()\n",
    "    pcd1.points = v3d(points1)\n",
    "    pcd2 = o3d.geometry\n",
    "    pcd2 = pcd2.PointCloud()\n",
    "    pcd2.points = v3d(points2)\n",
    "    o3d.visualization.draw_geometries(\n",
    "        [coord_1, coord_2, pcd1.paint_uniform_color([0, 1, 0]), pcd2.paint_uniform_color([0, 0, 1])]\n",
    "    )\n",
    "\n",
    "\n",
    "# Purpose: To construct the interative nearest points algorithm, create a\n",
    "# loop that connects correspondence discovery and procrustes alignment.\n",
    "# Do this until the correspondences haven't changed (i.e. till convergence).\n",
    "# Inputs:\n",
    "# pcd1: N x 3 matrix of points in starting point cloud\n",
    "# pcd2: N x 3 matrix of points in target point cloud\n",
    "# iterations: Regardless of convergence, the maximum number of iterations to do is\n",
    "# tolerance: general tolerance for the ICP\n",
    "# Returns: A tuple i.e. (newly formed, pcd1, original pcd2, transformation_matrix)\n",
    "def performICP(src, dst, coord_1, coord_2, iterations=100, tolerance=10 ** -15, freq=50):\n",
    "    transform = np.eye(4)\n",
    "    freq_idx = 0\n",
    "    curr_rmse = getRmse(src, dst)\n",
    "    visualisePcd(dst, src, coord_2, coord_1)\n",
    "    for idx in range(iterations):\n",
    "        corr = nearestNeighbour(src, dst)\n",
    "        mat = procrustesAlignment(dst, src, corr)\n",
    "        transform = mat @ transform\n",
    "        obtained_src = (\n",
    "            mat @ np.vstack((copy.deepcopy(src).T, np.ones((1, src.__len__()))))\n",
    "        )\n",
    "        src = np.true_divide(obtained_src.T[:, 0:3], obtained_src.T[:, [-1]])\n",
    "        coord_1 = coord_1.transform(mat)\n",
    "        if idx - freq_idx == freq:\n",
    "            freq_idx = idx\n",
    "            visualisePcd(dst, src, coord_2, coord_1)\n",
    "        src = src[corr, :]\n",
    "        new_rmse = getRmse(src, dst)\n",
    "        if idx > 0 and np.abs(new_rmse - curr_rmse) < tolerance:\n",
    "            print(\"Tolerance reached at iteration: \", idx)\n",
    "            break\n",
    "        curr_rmse = new_rmse\n",
    "    visualisePcd(dst, src, coord_2, coord_1)\n",
    "    print(\"Absolute Allignment Error reached = \", getRmse(src, dst))\n",
    "    return src, transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(pcd1)\n",
    "np.random.shuffle(pcd2)\n",
    "\n",
    "pcd2 += np.random.randn(pcd2.shape[0], dim) * .001\n",
    "print(\"Intial PCD = \", pcd1)\n",
    "print(\"Intial moved PCD i.e. from another frame= \", pcd2)\n",
    "print(\"Applied Transformation = \\n\", test_transformation)\n",
    "\n",
    "new_obtained_pcd, final_transform = performICP(pcd2, pcd1, coord_2, coord_1)\n",
    "\n",
    "print(\"Final Transformation Calculated = \\n\", final_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.inv(test_transformation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 2.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ICP does not always provide the correct alignment. When the correspondences are known, the mismatch can be caused by measurement error/noise in the points/point clouds. When the correspondences are unknown, the alignment may be inaccurate due to erroneous point/neighbor matching, especially if multiple points are close together or the two point clouds are separated by a big transformation. Furthermore, because reflections cannot be represented by pure rotations, the best available rotation will be used to approximate reflections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICP has numerous variations based on the methodologies utilised in one or more of the algorithm's stages:\n",
    "\n",
    "1) Choosing source points between the two meshes\n",
    "All points (robust), uniform sub-sampling, random sampling, normal sampling, and more variations are conceivable. Normal sampling appears to be the ideal option because it is straightforward, low-cost, and generally converges faster.\n",
    "\n",
    "2) Point matching between the two meshes (finding correspondences)\n",
    "Closest point (most robust), closest compatible point, regular shooting (best for some cases), normal shooting to a compatible point, projection (fastest), projection followed by search, and so on are some of the variations.\n",
    "\n",
    "3) Correspondence weighting\n",
    "Constant weight, lower weights for larger distance pairings, weights based on normal compatibility, weights based on uncertainty, and other variations are available. Regular compatability and uncertainty-based variations converge slightly faster than normal compatability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
